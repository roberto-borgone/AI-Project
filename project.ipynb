{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcuaFdKEzv0y",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Install requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryWt0frSztI_",
        "colab_type": "code",
        "outputId": "c78b2d16-b72c-47b4-fe31-c17200a3e846",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.4.2'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.3.1) (1.17.5)\n",
            "Requirement already satisfied: torchvision==0.4.2 in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: torch==1.3.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.17.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.4.2) (6.2.2)\n",
            "Requirement already satisfied: Pillow-SIMD in /usr/local/lib/python3.6/dist-packages (6.0.0.post0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot5KOZU-zwb3",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdWw9Oulzw5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import copy \n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "from torch.autograd import Function\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.models import alexnet\n",
        "from torchvision.datasets import VisionDataset\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFkGBFZrtcsr",
        "colab_type": "text"
      },
      "source": [
        "# **Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMhv9aVHtdfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_DIR = 'AIML_project/dataset1'\n",
        "NUM_CLASSES = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETuHFklY0dcp",
        "colab_type": "text"
      },
      "source": [
        "####**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssupi0gL0ecs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        " \n",
        "train_transform = transforms.Compose([transforms.Resize(256),      # Resizes short size of the PIL image to 256\n",
        "                                      transforms.CenterCrop(224),  # Crops a central square patch of the image\n",
        "                                                                  # 224 because torchvision's AlexNet needs a 224x224 input!\n",
        "                                                                  # Remember this when applying different transformations, otherwise you get an error\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize(mean, std)# Normalizes tensor with mean and standard deviation\n",
        "                                        \n",
        "])\n",
        "\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([transforms.Resize(256),\n",
        "                                    transforms.CenterCrop(224),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean, std)                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vov-bAAz5qa",
        "colab_type": "text"
      },
      "source": [
        "####**class PACS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NZDA3Eiz6IN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_EXTENSIONS = [\n",
        "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
        "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
        "]\n",
        "\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "class PACS(VisionDataset):\n",
        "  def __init__(self, root, split='train', transform=None, target_transform=None, loader=pil_loader):\n",
        "        super(PACS, self).__init__(root, transform=transform, target_transform=target_transform)\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.loader = loader\n",
        "        self.classes, self.class_to_idx = self._find_classes(self.root)\n",
        "        self.images = self.make_dataset(DATA_DIR,self.class_to_idx)\n",
        "\n",
        "  def _find_classes(self, dir):\n",
        "        \n",
        "        if sys.version_info >= (3, 5):\n",
        "            # Faster and available in Python 3.5 and above\n",
        "            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
        "        else:\n",
        "            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
        "        classes.sort()\n",
        "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
        "        return classes, class_to_idx\n",
        "\n",
        "  def make_dataset(self, dir, class_to_idx):\n",
        "    images = []\n",
        "    dir = os.path.expanduser(dir)\n",
        "    \n",
        "    for target in sorted(class_to_idx.keys()):\n",
        "        d = os.path.join(dir, target)\n",
        "        if not os.path.isdir(d):\n",
        "            continue\n",
        "        for root, dirs, _ in sorted(os.walk(d)):\n",
        "            for i in sorted(dirs):\n",
        "                path = os.path.join(root, i)\n",
        "                item = (path, class_to_idx[target])\n",
        "                images.append(item)\n",
        "\n",
        "    return images  # contiene i path delle cartelle contenenti le quaterne. Ogni path è associato ad un'etichetta che indica la posizione dell'odd\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "\n",
        "        quad = []\n",
        "        sample_dir, label = self.images[index]\n",
        "        for img in sorted(os.listdir(sample_dir)):\n",
        "          image_path = os.path.join(sample_dir, img) \n",
        "          quad.append( self.loader(image_path))\n",
        "\n",
        "        # Applies preprocessing when accessing the image\n",
        "        if self.transform is not None:\n",
        "          for i in range(4):\n",
        "              quad[i] = self.transform(quad[i])\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "\n",
        "        return quad, label  \n",
        "\n",
        "  def __len__(self):\n",
        "        length = len(self.images)\n",
        "        return length  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T-c0nl7wBRW",
        "colab_type": "text"
      },
      "source": [
        "####**OOONet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYoJ1c_gwBtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConcatLayer(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, fc6_1 ,fc6_2, fc6_3 ,fc6_4):\n",
        "        print(fc6_1.shape,fc6_2.shape,fc6_3.shape,fc6_4.shape )\n",
        "        concatenation = torch.cat([fc6_1, fc6_2,fc6_3,fc6_4], dim=1) # esempio (3,4) (3,4)  ---> ( 2, 3, 4 )  oppure ( 4096 )( 4096 ) --> (2,4096)\n",
        "        #return concatenation.view_as(concatenation)\n",
        "        print(concatenation.shape)\n",
        "        return concatenation\n",
        "        \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output\n",
        "\n",
        "\n",
        "class OOONet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(OOONet, self).__init__()\n",
        "        \n",
        "        self.branch1_1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.AdaptiveAvgPool2d((6, 6)),           \n",
        "        )\n",
        "        self.branch1_2 = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),                   #FC 6 \n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.branch2_1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.AdaptiveAvgPool2d((6, 6)),\n",
        "        )\n",
        "        self.branch2_2 = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),                   #FC 6 \n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.branch3_1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.AdaptiveAvgPool2d((6, 6)),   \n",
        "        )\n",
        "        self.branch3_2 = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),                   #FC 6 \n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.branch4_1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  #CONV 5 \n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.AdaptiveAvgPool2d((6, 6)),\n",
        "               \n",
        "        )\n",
        "        self.branch4_2 = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),                   #FC 6 \n",
        "            nn.ReLU(inplace=True),\n",
        "        )        \n",
        "        #self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "     \n",
        "        #Livelli di fusione!!!!!!!\n",
        "        self.concatLayer = ConcatLayer()\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096,16384),            # 16384 da 4096\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, NUM_CLASSES),\n",
        "        )\n",
        "        \n",
        "    \n",
        "    def forward(self, x):\n",
        "        #bisogna definire X perchè dovrebbe essere un vettore di 4 immagini.\n",
        "        x0 = self.branch1_1(x[0])\n",
        "        #x0 = self.avgpool(x0)\n",
        "        x0 = torch.flatten(x0, 1)\n",
        "        output_branch1 = self.branch1_2(x0)\n",
        "\n",
        "        x1 = self.branch2_1(x[1])\n",
        "        #x1 = self.avgpool(x1)\n",
        "        x1 = torch.flatten(x1, 1)\n",
        "        output_branch2 = self.branch2_2(x1)\n",
        "\n",
        "        x2 = self.branch3_1(x[2])\n",
        "        #x2 = self.avgpool(x2)\n",
        "        x2 = torch.flatten(x2, 1)\n",
        "        output_branch3 = self.branch3_2(x2)\n",
        "\n",
        "        x3 = self.branch4_1(x[3])\n",
        "        #x3 = self.avgpool(x3)\n",
        "        x3 = torch.flatten(x3, 1)\n",
        "        output_branch4 = self.branch4_2(x3)\n",
        "\n",
        "        out = self.concatLayer.apply(output_branch1,output_branch2,output_branch3,output_branch4) \n",
        "        #out = torch.flatten(out,1)\n",
        "        print(\"out\")\n",
        "        print(out.shape)\n",
        "        out = self.classifier(out)\n",
        "      \n",
        "        \n",
        "        return out\n",
        "\n",
        "def buildO3Net ():\n",
        "\n",
        "    model = alexnet(pretrained=True)\n",
        "\n",
        "    net =  OOONet()\n",
        "    #0,3,6,8,10,14\n",
        "\n",
        "    #DEEP COPY FEATURES OF BRANCH 1\n",
        "\n",
        "    net.branch1_1[0].weight.data = copy.deepcopy(model.features[0].weight.data)\n",
        "    net.branch1_1[0].bias.data = copy.deepcopy(model.features[0].bias.data)   \n",
        "    net.branch1_1[3].weight.data = copy.deepcopy(model.features[3].weight.data)\n",
        "    net.branch1_1[3].bias.data = copy.deepcopy(model.features[3].bias.data)\n",
        "    net.branch1_1[6].weight.data = copy.deepcopy(model.features[6].weight.data)\n",
        "    net.branch1_1[6].bias.data = copy.deepcopy(model.features[6].bias.data)\n",
        "    net.branch1_1[8].weight.data = copy.deepcopy(model.features[8].weight.data)\n",
        "    net.branch1_1[8].bias.data = copy.deepcopy(model.features[8].bias.data)\n",
        "    net.branch1_1[10].weight.data = copy.deepcopy(model.features[10].weight.data)\n",
        "    net.branch1_1[10].bias.data = copy.deepcopy(model.features[10].bias.data)\n",
        "    net.branch1_2[1].weight.data = copy.deepcopy(model.classifier[1].weight.data)\n",
        "    net.branch1_2[1].bias.data = copy.deepcopy(model.classifier[1].bias.data)\n",
        "\n",
        "\n",
        "    #DEEP COPY FEATURES OF BRANCH 2\n",
        "\n",
        "    net.branch2_1[0].weight.data = copy.deepcopy(model.features[0].weight.data)\n",
        "    net.branch2_1[0].bias.data = copy.deepcopy(model.features[0].bias.data)   \n",
        "    net.branch2_1[3].weight.data = copy.deepcopy(model.features[3].weight.data)\n",
        "    net.branch2_1[3].bias.data = copy.deepcopy(model.features[3].bias.data)\n",
        "    net.branch2_1[6].weight.data = copy.deepcopy(model.features[6].weight.data)\n",
        "    net.branch2_1[6].bias.data = copy.deepcopy(model.features[6].bias.data)\n",
        "    net.branch2_1[8].weight.data = copy.deepcopy(model.features[8].weight.data)\n",
        "    net.branch2_1[8].bias.data = copy.deepcopy(model.features[8].bias.data)\n",
        "    net.branch2_1[10].weight.data = copy.deepcopy(model.features[10].weight.data)\n",
        "    net.branch2_1[10].bias.data = copy.deepcopy(model.features[10].bias.data)\n",
        "    net.branch2_2[1].weight.data = copy.deepcopy(model.classifier[1].weight.data)\n",
        "    net.branch2_2[1].bias.data = copy.deepcopy(model.classifier[1].bias.data)\n",
        "\n",
        "    #DEEP COPY FEATURES OF BRANCH 3\n",
        "\n",
        "    net.branch3_1[0].weight.data = copy.deepcopy(model.features[0].weight.data)\n",
        "    net.branch3_1[0].bias.data = copy.deepcopy(model.features[0].bias.data)   \n",
        "    net.branch3_1[3].weight.data = copy.deepcopy(model.features[3].weight.data)\n",
        "    net.branch3_1[3].bias.data = copy.deepcopy(model.features[3].bias.data)\n",
        "    net.branch3_1[6].weight.data = copy.deepcopy(model.features[6].weight.data)\n",
        "    net.branch3_1[6].bias.data = copy.deepcopy(model.features[6].bias.data)\n",
        "    net.branch3_1[8].weight.data = copy.deepcopy(model.features[8].weight.data)\n",
        "    net.branch3_1[8].bias.data = copy.deepcopy(model.features[8].bias.data)\n",
        "    net.branch3_1[10].weight.data = copy.deepcopy(model.features[10].weight.data)\n",
        "    net.branch3_1[10].bias.data = copy.deepcopy(model.features[10].bias.data)\n",
        "    net.branch3_2[1].weight.data = copy.deepcopy(model.classifier[1].weight.data)\n",
        "    net.branch3_2[1].bias.data = copy.deepcopy(model.classifier[1].bias.data)\n",
        "\n",
        "    #DEEP COPY FEATURES OF BRANCH 4\n",
        "\n",
        "    net.branch4_1[0].weight.data = copy.deepcopy(model.features[0].weight.data)\n",
        "    net.branch4_1[0].bias.data = copy.deepcopy(model.features[0].bias.data)   \n",
        "    net.branch4_1[3].weight.data = copy.deepcopy(model.features[3].weight.data)\n",
        "    net.branch4_1[3].bias.data = copy.deepcopy(model.features[3].bias.data)\n",
        "    net.branch4_1[6].weight.data = copy.deepcopy(model.features[6].weight.data)\n",
        "    net.branch4_1[6].bias.data = copy.deepcopy(model.features[6].bias.data)\n",
        "    net.branch4_1[8].weight.data = copy.deepcopy(model.features[8].weight.data)\n",
        "    net.branch4_1[8].bias.data = copy.deepcopy(model.features[8].bias.data)\n",
        "    net.branch4_1[10].weight.data = copy.deepcopy(model.features[10].weight.data)\n",
        "    net.branch4_1[10].bias.data = copy.deepcopy(model.features[10].bias.data)\n",
        "    net.branch4_2[1].weight.data = copy.deepcopy(model.classifier[1].weight.data)\n",
        "    net.branch4_2[1].bias.data = copy.deepcopy(model.classifier[1].bias.data)\n",
        "\n",
        "    #DEEP COPY OF LAST TWO FC LAYERS\n",
        "\n",
        "    net.classifier[1].weight.data = copy.deepcopy(model.classifier[4].weight.data)\n",
        "    net.classifier[1].bias.data = copy.deepcopy(model.classifier[4].bias.data)\n",
        "   \n",
        "    return net\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbEcuheuxq7N",
        "colab_type": "text"
      },
      "source": [
        "####**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRfhXYeuxrsI",
        "colab_type": "code",
        "outputId": "916c1243-8685-488c-97d6-bb493c507466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "# Clone github repository with data\n",
        "if not os.path.isdir('./AIML_project'):\n",
        "  !git clone https://github.com/rebeccapelaca/AIML_project.git\n",
        "\n",
        "dataset = PACS(DATA_DIR, transform=train_transform)\n",
        "print('Dataset: {}'.format(len(dataset)))\n",
        "\n",
        "train_indexes = [idx for idx in range(len(dataset)) if (idx % 3) == 1]\n",
        "val_indexes = [idx for idx in range(len(dataset)) if (idx % 3) == 2]\n",
        "test_indexes = [idx for idx in range(len(dataset)) if not (idx % 3)]\n",
        "\n",
        "train_dataset = Subset(dataset, train_indexes)\n",
        "val_dataset = Subset(dataset, val_indexes)\n",
        "test_dataset = Subset(dataset, test_indexes)\n",
        "\n",
        "print('Training Dataset: {}'.format(len(train_dataset)))\n",
        "print('Validation Dataset: {}'.format(len(val_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 14064\n",
            "Training Dataset: 4688\n",
            "Validation Dataset: 4688\n",
            "Test Dataset: 4688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TUC2xBox3Ul",
        "colab_type": "text"
      },
      "source": [
        "####**Dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2cSUwK6xkCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=256, shuffle=True, num_workers=4, drop_last=True)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size=256, shuffle=True, num_workers=4, drop_last=True)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=256, shuffle=True, num_workers=4, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdzn9gS6Igls",
        "colab_type": "text"
      },
      "source": [
        "####**Set Arguments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDPsHDf1Ikeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
        " \n",
        "\n",
        "#BATCH_SIZE = 256     # Batch size will be chosen through a grid search\n",
        "LR = 1e-3            # Learning rate will be chosen through a grid search\n",
        "\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
        "\n",
        "#NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = 10\n",
        "\n",
        "#Grid search parameters\n",
        "lrates=[1e-4,1e-3]       #tried 1e-6 (too low),1e-5(too low),1e-4,3e-4,7e-4,1e-3(too high with Adam),1e-2(too high)\n",
        "batch_sizes=[192,256]   #128(too small),192,256,320,384(too big)\n",
        "NUM_EPOCHS=[30] #30 is the best"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvLJCfwJyTGb",
        "colab_type": "text"
      },
      "source": [
        "####**Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UucjCgoqyVgp",
        "colab_type": "code",
        "outputId": "fdf47598-badd-4f76-c31f-a36813a91796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "net = buildO3Net()\n",
        "net.to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
        "parameters_to_optimize = net.parameters()\n",
        "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "num_epochs = 30\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, num_epochs, scheduler.get_lr()))\n",
        "\n",
        "    for images, labels in train_dataloader:\n",
        "      # Bring data over the device of choice\n",
        "      for i in range(4):\n",
        "        images[i] = images[i].to(DEVICE)\n",
        "      \n",
        "      labels = labels.to(DEVICE)\n",
        "\n",
        "      net.train() # Sets module in training mode\n",
        "\n",
        "      # PyTorch, by default, accumulates gradients after each backward pass\n",
        "      # We need to manually set the gradients to zero before starting a new iteration\n",
        "      optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "      # Forward pass to the network\n",
        "      outputs = net(images)\n",
        "\n",
        "      # Compute loss based on output and ground truth\n",
        "      loss = criterion(outputs, labels)\n",
        "      print(\"loss:{}\".format(loss.item()))\n",
        "\n",
        "      # Compute gradients for each layer and update weights\n",
        "      loss.backward()  # backward pass: computes gradients\n",
        "      optimizer.step() # update weights based on accumulated gradients\n",
        "\n",
        "      current_step += 1\n",
        "    \n",
        "   \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1/30, LR = [0.001]\n",
            "torch.Size([256, 4096]) torch.Size([256, 4096]) torch.Size([256, 4096]) torch.Size([256, 4096])\n",
            "torch.Size([256, 16384])\n",
            "out\n",
            "torch.Size([256, 16384])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c4732d298a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;31m# Forward pass to the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0;31m# Compute loss based on output and ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-28d1acffd2a4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"out\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [256 x 16384], m2: [4096 x 4096] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:290"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsNFT9HwJcm-",
        "colab_type": "text"
      },
      "source": [
        "###**Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz7DUMOHJed6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = net.to(DEVICE) \n",
        "net.train(False) # Set Network to evaluation mode , equivalent to net.eval()\n",
        "#test_dataloader = DataLoader(test_dataset, batch_size=best_batch, shuffle=True, num_workers=4)\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(test_dataloader): #evaluate performance on validation set\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = net(images)\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy on validation set\n",
        "accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "\n",
        "print('Test Accuracy: {}'.format(accuracy))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}